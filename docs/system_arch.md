# System Architecture

## Overview

The domain adaptation methods we're going to establish include the following ones.

 - **Desity Ratio Estimation**: in such methods, we can re-weight the source domain data to make it closer to the target domain distribution.
 To do so, we need to support re-weighting functions which changes the weights of samples when calculating the loss.

 - **Pseudo Labeling**: in such methods, we can give pseudo labels to the data sampled from the target domain (i.e. the behavioral policy) and use them to align the policy LLM.
The pseudo labels can be generated in serverl different ways.
   - By reward model: we can use a reward model trained on the source domain data to generate pseudo labels for the target domain data.
   To do so, we need to support the reward model training and inference.
   - By lanugage model: we can prompt a langauge model, e.g. GPT-4, to label the responses generated by the target domain policy.
    To do so, we need to support the language model inference, or API calls to language model service providers.

 - **Combination of the above two**: we can combine the above two methods to further improve the performance.

 - Other kinds of methods will be added later

## Components

Now, let's take a look at the major components of the project.

### Data loader

The original [HALOs project](https://github.com/ContextualAI/HALOs) provides a very nice architecture for data loading.
To achieve our research aim, there's no need to change their data loading module.
So, we just summarize the major types of their loaders as follows.

 - `SFTDataLoader`: used to load data for SFT. The returned data dictionary is of the format `{'prompt_text': str, 'target_text': str, 'prompt_token_ids': ArrayLike, 'prompt_attention_mask': ArrayLike, 'target_token_ids': ArrayLike, 'target_attention_mask': ArrayLike}`.
 - `UnpairedPreferenceDataLoader`: used to load unpaired preference data. The returned data dictionary is of the format `{'prompt_text': str, 'target_text': str, 'prompt_token_ids': ArrayLike, 'prompt_attention_mask': ArrayLike, 'target_token_ids': ArrayLike, 'target_attention_mask': ArrayLike, 'status': Union['chosen', 'rejected']}`.
 - `PairedPreferenceDataLoader`: used to load pairwise preference data. The returned data dictionary is of the format `{'prompt_text': str, 'chosen_text': str, 'prompt_token_ids': ArrayLike, 'prompt_attention_mask': ArrayLike, 'chosen_token_ids': ArrayLike, 'chosen_attention_mask': ArrayLike, 'rejected_token_ids': ArrayLike, 'rejected_attention_mask': ArrayLike,}`.

### Model

The original [HALOs project](https://github.com/ContextualAI/HALOs) loads all models directly from the HuggingFace model hub.
Hereby, we emphasize some specific classes in the `models.py` script.

- `AutoModelForCausalLMWithValueHead`: this is used to create a reward model through appending a value head to a causal language model.
- `ValueHead`: the helper class for the `AutoModelForCausalLMWithValueHead` class.

### Trainer

The trainer classes in `trainers.py` are self-explanatory.
The modification we need to make is to support the following features:

- re-weighting functions in the loss calculation
- pseudo label generation

## Preference Models

The following types of preference functions shall be supported.

 - `PointwisePreferenceFunction`: the pointwise preference function, which takes a single response as input and outputs a preference score.
    - `OfflinePointwisePreferenceFunction`: input is a response, output is a boolean variable to indicate whether it is a desirable one.
    - `RMScorePreferenceFunction`: input is a response, output is a real-valued score, evaluated by a reward model trained on the source domain data.
    - `RMBinaryPreferenceFunction`: input is a response, output is a binary  (0 for undesirable response, 1 for desirable response), evaluated by a reward model trained on the source domain data.
    - `LMBinaryPreferenceFunction`: input is a response, output is a binary  (0 for undesirable response, 1 for desirable response), evaluated by a language model.
 - `PairwisePreferenceFunction`: the pairwise preference function, which takes a pair of responses as input and outputs the `chosen_response` and the `rejected_response`.
    - `OfflinePairwisePreferenceFunction`: input is a pair of responses, outputs are the offline responses from the source domain data, i.e. `chosen_response` and `rejected_response`.
    - `RMPairwisePreferenceFunction`: input is a pair of responses, outputs are the `chosen_responses` and `rejected_responses`, evaluated by a reward model trained on the source domain data.
    - `LMPairwisePreferenceFunction`: input is a pair of responses, outputs are the `chosen_responses` and `rejected_responses`, evaluated by a language model.


## Labelling Language Model

To support the online annotation for the LM-based preference functions descriped in the previous section, we need to support LLMs of the following differenct types.

 - `APIBasedLLM`: the LLM which is based on the API calls to language model service providers.
 - `InferenceBasedLLM`: the LLM which is based on the inference of a pretrained language model.


### Evaluation

TO BE ADDED

## Data Flow

To train the policies, for a given source domain dataset $\mathbb{D}$, the data flow in a single training iteration during the *alignment* (e.g. RLHF or DPO) is descriped as follows.
Hereby, we assume that batch size is $|B|$.

  1. Exact a batch of data $\mathbb{B}=\{(x_i, y_i^+, y_i^-)\}_{i=1}^{|B|}$ from $\mathbb{D}$.
  2. If the preference function is a pointwise one, goto step 3; otherwise goto step 4.
  3. Annotate 